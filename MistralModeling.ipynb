{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import llama_cpp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge graph initialized.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the knowledge graph as a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Debug message\n",
    "print(\"Knowledge graph initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf:\n",
    "            for page_num in range(pdf.page_count):\n",
    "                page = pdf[page_num]\n",
    "                text += page.get_text(\"text\")\n",
    "        print(\"Successfully extracted text from PDF.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file: {e}\")\n",
    "        text = \"\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted text from PDF.\n",
      "Activity input set successfully: Provide a detailed 7-day mobile app development plan. Outline each day’s tasks, key\n",
      "milestones, and ...\n"
     ]
    }
   ],
   "source": [
    "# Input cell for activity source\n",
    "activity_input = None  # Placeholder for activity text\n",
    "pdf_path = \"file.pdf\"  # Set PDF path here if using a PDF, or leave as an empty string\n",
    "\n",
    "# Full task description as text input for dynamic activity setting\n",
    "text_input = (\n",
    "    \"Provide a detailed 7-day mobile app development plan. Outline each day’s tasks, key milestones, and expected deliverables. \"\n",
    "    \"Include a brief risk assessment with potential risks and mitigation strategies. Keep responses concise and brief.\"\n",
    ")\n",
    "\n",
    "# Conditional logic to determine whether to use PDF or text input\n",
    "use_pdf = pdf_path != \"\"\n",
    "\n",
    "# Set activity_input based on whether to use PDF or text input\n",
    "if use_pdf:\n",
    "    activity_input = extract_text_from_pdf(pdf_path)\n",
    "    if not activity_input:\n",
    "        print(\"Warning: PDF text extraction failed, falling back to text input.\")\n",
    "        activity_input = text_input\n",
    "else:\n",
    "    activity_input = text_input\n",
    "\n",
    "print(\"Activity input set successfully:\", activity_input[:100] + \"...\" if len(activity_input) > 100 else activity_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted text from PDF.\n",
      "Activity input set successfully: Provide a detailed 7-day mobile app development plan. Outline each day’s tasks, key\n",
      "milestones, and ...\n"
     ]
    }
   ],
   "source": [
    "# Set activity_input based on whether to use PDF or text input\n",
    "if use_pdf:\n",
    "    activity_input = extract_text_from_pdf(pdf_path)\n",
    "    if not activity_input:\n",
    "        print(\"Warning: PDF text extraction failed, falling back to text input.\")\n",
    "        activity_input = text_input\n",
    "else:\n",
    "    activity_input = text_input\n",
    "\n",
    "print(\"Activity input set successfully:\", activity_input[:100] + \"...\" if len(activity_input) > 100 else activity_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gnerating Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody_of_knowledge_with_embeddings_saved (1).csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Populate the graph with nodes and edges based on the dataset\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Add process as nodes with a default name if missing\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     process_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m     process_name \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess_Name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnnamed_Process_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocess_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "df=\"body_of_knowledge_with_embeddings_saved (1).csv\"\n",
    "# Populate the graph with nodes and edges based on the dataset\n",
    "for idx, row in df.iterrows():\n",
    "    # Add process as nodes with a default name if missing\n",
    "    process_id = row[\"Process_ID\"]\n",
    "    process_name = row.get(\"Process_Name\", f\"Unnamed_Process_{process_id}\")\n",
    "    knowledge_area = row[\"Knowledge_Area_Name\"]\n",
    "    process_description = row[\"Process_Description\"]\n",
    "    \n",
    "    # Node attributes include process details for reasoning\n",
    "    G.add_node(process_id, name=process_name, area=knowledge_area, description=process_description)\n",
    "    \n",
    "    # Manually parse 'Inputs' for dependencies, if 'Inputs' is a string\n",
    "    inputs = []\n",
    "    if isinstance(row[\"Inputs\"], str):\n",
    "        try:\n",
    "            inputs = parse_inputs(row[\"Inputs\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing Inputs for row {idx}: {e}\")\n",
    "            inputs = []\n",
    "\n",
    "    # Ensure each parsed input is hashable and add edges\n",
    "    for input_item in inputs:\n",
    "        if isinstance(input_item, str) or isinstance(input_item, int):  # Check if input is hashable\n",
    "            G.add_edge(input_item, process_id, relationship=\"dependency\")\n",
    "        else:\n",
    "            print(f\"Skipped non-hashable input for process '{process_name}': {input_item}\")\n",
    "\n",
    "    # Debug message for node and edge addition\n",
    "    print(f\"Added process '{process_name}' with dependencies: {inputs if inputs else 'None'}\")\n",
    "\n",
    "print(\"Knowledge graph populated with processes and dependencies.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoing logic for LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning functions initialized.\n"
     ]
    }
   ],
   "source": [
    "# Define function to get dependencies of a given process\n",
    "def get_dependencies(process_id):\n",
    "    dependencies = list(G.predecessors(process_id))\n",
    "    print(f\"Dependencies for '{G.nodes[process_id]['name']}': {dependencies}\")\n",
    "    return dependencies\n",
    "\n",
    "# Define function to get stakeholders for a process (based on edge relationships)\n",
    "def get_stakeholders(process_id):\n",
    "    stakeholders = [node for node, attr in G.nodes(data=True) if attr.get(\"area\") == \"Stakeholder Management\"]\n",
    "    print(f\"Stakeholders for '{G.nodes[process_id]['name']}': {stakeholders}\")\n",
    "    return stakeholders\n",
    "\n",
    "# Define function to identify risk-related dependencies (mocked for simplicity)\n",
    "def get_risks(process_id):\n",
    "    risks = [node for node, attr in G.nodes(data=True) if attr.get(\"area\") == \"Risk Management\"]\n",
    "    print(f\"Risks affecting '{G.nodes[process_id]['name']}': {risks}\")\n",
    "    return risks\n",
    "\n",
    "# Debugging statements to verify function outputs\n",
    "print(\"Reasoning functions initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for Llama to query the graph\n",
    "def generate_graph_context(activity_input):\n",
    "    # Find the process ID associated with the activity input\n",
    "    process_ids = [pid for pid, data in G.nodes(data=True) if data['name'].lower() in activity_input.lower()]\n",
    "    if not process_ids:\n",
    "        print(\"No matching process found in the knowledge graph.\")\n",
    "        return \"No relevant process found in the knowledge graph.\"\n",
    "\n",
    "    process_id = process_ids[0]  # Assuming the first match\n",
    "    dependencies = get_dependencies(process_id)\n",
    "    stakeholders = get_stakeholders(process_id)\n",
    "    risks = get_risks(process_id)\n",
    "\n",
    "    # Construct context for Llama's input prompt\n",
    "    context = (\n",
    "        f\"Activity: {activity_input}\\n\"\n",
    "        f\"Dependencies: {dependencies}\\n\"\n",
    "        f\"Stakeholders: {stakeholders}\\n\"\n",
    "        f\"Risks: {risks}\\n\"\n",
    "    )\n",
    "\n",
    "    # Debug message for constructed context\n",
    "    print(f\"Constructed graph context for Llama:\\n{context}\")\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing data and model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 35 key-value pairs and 147 tensors from Llama-3.2-1B-Instruct-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-1B-Instruct-GGU...\n",
      "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 112\n",
      "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type q5_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 16\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 1.24 B\n",
      "llm_load_print_meta: model size       = 861.81 MiB (5.85 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3.2 1B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =   861.81 MiB\n",
      "...........................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    16.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   16.00 MiB, K (f16):    8.00 MiB, V (f16):    8.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   254.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 518\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'Llama 3.2 1B Instruct', 'general.architecture': 'llama', 'general.type': 'model', 'llama.block_count': '16', 'general.basename': 'Llama-3.2', 'general.finetune': 'Instruct', 'general.size_label': '1B', 'general.license': 'llama3.2', 'llama.context_length': '131072', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '8192', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.entries_count': '112', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '64', 'llama.attention.value_length': '64', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '64', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'quantize.imatrix.chunks_count': '125', 'quantize.imatrix.file': '/models_out/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Dataset loaded successfully. Sample data:\n",
      "                                              Prompt  \\\n",
      "0         What type of resource is the project team?   \n",
      "1  Create a WBS, or work breakdown structure, for...   \n",
      "2                         Select a type of resource.   \n",
      "3  Select the project management process that is ...   \n",
      "4            Define the project start and end dates.   \n",
      "\n",
      "                                          Completion  \n",
      "0                                     Human resource  \n",
      "1  A work breakdown structure (WBS) is a hierarch...  \n",
      "2                                     Human resource  \n",
      "3                                            Execute  \n",
      "4                     1. Determine the project scope  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import llama_cpp  # Ensure llama_cpp is installed and configured\n",
    "\n",
    "# Specify paths for the model and dataset\n",
    "model_path = os.path.join(\"Llama-3.2-1B-Instruct-Q5_K_M.gguf\")\n",
    "dataset_path = os.path.join(\"pmbok_prompt_completion_pairs (1).csv\")\n",
    "\n",
    "# Initialize the llama-cpp model with the specified model path\n",
    "llama = llama_cpp.Llama(model_path=model_path, verbose=True)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# Verify the data and model loading\n",
    "print(\"Model loaded successfully.\")\n",
    "print(\"Dataset loaded successfully. Sample data:\")\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_prompt_completion(example):\n",
    "    prompt = example[\"Prompt\"]\n",
    "    completion = example[\"Completion\"]\n",
    "    return f\"{prompt}\\n{completion}\"\n",
    "\n",
    "# Convert the dataset into prompt-completion pairs\n",
    "formatted_prompts = [prepare_prompt_completion(row) for _, row in dataset.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning Llama-3.2-1B Qunatized version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     768.17 ms /    27 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 75 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  is the project team.\n",
      "What type of resource is the project team?\n",
      "Human resource\n",
      "Training Step 2/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1175.70 ms /    90 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 8 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  smaller components, identifying the specific tasks, and describing their responsibilities.\n",
      "\n",
      "## Step \n",
      "Training Step 3/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     572.02 ms /    23 tokens\n",
      "Llama.generate: 2 prefix-match hit, remaining 20 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "Technology resource\n",
      "Physical resource\n",
      "Environmental resource\n",
      "\n",
      "## Step 1: Identify\n",
      "Training Step 4/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     689.67 ms /    35 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 14 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  the plan. Monitor and control the plan to ensure it is on track and that\n",
      "Training Step 5/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     654.69 ms /    29 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 43 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  and deliverables.\n",
      "2. Create a detailed project schedule.\n",
      "3. Estimate the\n",
      "Training Step 6/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    43 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     900.67 ms /    58 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 18 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  This is because each activity in the diagram represents a task that needs to be completed\n",
      "Training Step 7/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     676.80 ms /    33 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 20 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Project\n",
      "Procurement Project\n",
      "Procurement Project\n",
      "Implementation Project\n",
      "Procurement Project\n",
      "Training Step 8/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     681.28 ms /    35 tokens\n",
      "Llama.generate: 2 prefix-match hit, remaining 22 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: : An activity is a specific task or a set of tasks that need to be\n",
      "Training Step 9/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     703.99 ms /    37 tokens\n",
      "Llama.generate: 2 prefix-match hit, remaining 11 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  (1) Define the project scope and objectives; (2) Identify the stakeholders\n",
      "Training Step 10/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     609.89 ms /    26 tokens\n",
      "Llama.generate: 3 prefix-match hit, remaining 59 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "In the context of risk management, the first step is to identify risks.\n",
      "Training Step 11/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1027.96 ms /    74 tokens\n",
      "Llama.generate: 3 prefix-match hit, remaining 69 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  \n",
      "\n",
      "1. Resource allocation and management: These tools enable teams to assign resources to\n",
      "Training Step 12/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1114.72 ms /    84 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  \n",
      "\n",
      "The scope statement typically includes the following information:\n",
      "\n",
      "* Project description\n",
      "* Scope\n",
      "Training Step 13/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     729.28 ms /    44 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 50 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  1) identify the project's scope, 2) gather relevant data and\n",
      "Training Step 14/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    50 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     920.13 ms /    65 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 19 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  In other words, a sponsor is someone who is giving you the resources to get\n",
      "Training Step 15/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     633.97 ms /    34 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "Government regulations\n",
      "Peer pressure\n",
      "Peer evaluation\n",
      "Business environment\n",
      "Peer pressure\n",
      "\n",
      "Training Step 16/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     653.13 ms /    36 tokens\n",
      "Llama.generate: 3 prefix-match hit, remaining 54 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: .\n",
      "In this example, the phrase \"Plan the plan\" is an idiom\n",
      "Training Step 17/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     978.60 ms /    69 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  The objective is to create a project that is completed successfully on time, within budget\n",
      "Training Step 18/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1117.07 ms /    85 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 12 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  the project's progress.\n",
      "\n",
      "Key responsibilities of a project manager include:\n",
      "\n",
      "1. Exec\n",
      "Training Step 19/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     612.53 ms /    27 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  The following is a risk probability assessment of an event.\n",
      "\\begin{tabular\n",
      "Training Step 20/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    42 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     882.75 ms /    57 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 67 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  By establishing a baseline, project sponsors and stakeholders can assess the project's progress and\n",
      "Training Step 21/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1108.58 ms /    82 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  management plan is usually written by the project sponsor and the project manager, and it\n",
      "Training Step 22/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     791.54 ms /    44 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 86 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: .\n",
      "B) Earned value analysis is used to measure the performance of a project\n",
      "Training Step 23/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1342.23 ms /   101 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  should be categorized into three levels: best practice, significant, and minor. The\n",
      "Training Step 24/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1267.23 ms /    85 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 38 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Charter is developed.\n",
      "Project Scope Statement: This is a document that defines the project\n",
      "Training Step 25/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    38 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     905.97 ms /    53 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 49 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Project closure involves the completion of a project's requirements, meeting project schedules, and\n",
      "Training Step 26/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    49 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     940.26 ms /    64 tokens\n",
      "Llama.generate: 4 prefix-match hit, remaining 10 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  (1) assessing potential risks, (2) identifying vulnerabilities, (3)\n",
      "Training Step 27/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     564.84 ms /    25 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 55 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "\n",
      "Project Charter is the initial document that outlines the objectives, scope, and stakeholders\n",
      "Training Step 28/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     969.48 ms /    70 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 26 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  In general, stakeholders can be categorized into several types, including:\n",
      "1. **\n",
      "Training Step 29/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     720.63 ms /    41 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 17 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: . \n",
      "The best answer is A) Planning, Executing, Controlling,\n",
      "Training Step 30/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     669.52 ms /    32 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 32 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  is a structured approach to project planning that divides the project into manageable components. A\n",
      "Training Step 31/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     780.10 ms /    47 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 14 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  A factor that can make the project environment more stable is a factor that can make\n",
      "Training Step 32/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     588.17 ms /    28 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "Transition\n",
      "Stabilization\n",
      "Closure\n",
      "\n",
      "The best answer is A\n",
      "Training Step 33/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    42 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     865.35 ms /    57 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 25 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Project risks are typically identified and managed through a risk management plan.\n",
      "\n",
      "Here's an\n",
      "Training Step 34/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     706.74 ms /    40 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 38 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Explores the potential impact on the organization's operations, customers, and the\n",
      "Training Step 35/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    38 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     825.17 ms /    53 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 11 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Develop a plan for implementing the baseline in a production environment.\n",
      "\n",
      "## Step 1\n",
      "Training Step 36/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     598.97 ms /    26 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 25 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: .\n",
      "The project will be implemented within the following time frames:\n",
      "* **Short-term\n",
      "Training Step 37/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     764.82 ms /    40 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 22 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  It includes detailed information about the project, its objectives, deliverables, timelines,\n",
      "Training Step 38/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     679.85 ms /    37 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 35 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Here are some suggestions:\n",
      "1. **Point-based system**: Implement a point-based\n",
      "Training Step 39/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     834.07 ms /    50 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 25 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  This may include employees, customers, vendors, regulatory bodies, investors, or any\n",
      "Training Step 40/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     721.24 ms /    40 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 36 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Improved Communication and Collaboration: Standardizing on a methodology encourages communication and collaboration among team\n",
      "Training Step 41/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    36 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     865.68 ms /    51 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 22 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "\n",
      "## Step 1: Understanding the question\n",
      "The question asks about the term\n",
      "Training Step 42/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     731.97 ms /    37 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 46 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  A) It assists in organizing tasks into smaller activities, thereby making the project more\n",
      "Training Step 43/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    46 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1012.53 ms /    61 tokens\n",
      "Llama.generate: 4 prefix-match hit, remaining 71 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  A project management process is an organized framework that outlines the steps, roles, and\n",
      "Training Step 44/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.04 ms /    86 tokens\n",
      "Llama.generate: 4 prefix-match hit, remaining 59 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  scope, budget, timelines, resources, and risks, while a project schedule outlines\n",
      "Training Step 45/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1079.33 ms /    74 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 47 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  By doing so, it helps in ensuring that all stakeholders are informed and engaged effectively\n",
      "Training Step 46/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    47 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     930.03 ms /    62 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 74 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Here's a step-by-step guide to creating a project schedule:\n",
      "\n",
      "1. **\n",
      "Training Step 47/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1190.73 ms /    89 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 41 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  changes, the impact of the changes on the project, and the timeline of the\n",
      "Training Step 48/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    41 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     883.48 ms /    56 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 9 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  It involves developing a detailed quality plan, establishing quality metrics, and allocating resources accordingly\n",
      "Training Step 49/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     602.34 ms /    24 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "Risk response strategies involve identifying, evaluating, and mitigating potential risks to an\n",
      "Training Step 50/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1217.58 ms /    87 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 13 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  to be completed in a project. It may be a step within a larger project\n",
      "Training Step 51/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     612.54 ms /    28 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 65 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  for the week of February 1, 2021, to ensure everyone is\n",
      "Training Step 52/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1229.95 ms /    80 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 54 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  It helps to ensure that projects are well-planned, executed, and delivered on\n",
      "Training Step 53/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1054.55 ms /    69 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Here are the different types of project phases:\n",
      "\n",
      "1. **Initiation Phase**:\n",
      "Training Step 54/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1223.06 ms /    88 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 50 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  The goal of the WBS is to provide a hierarchical, logical structure for organizing\n",
      "Training Step 55/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    50 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1046.37 ms /    65 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 75 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  The WBS is used to break down the large project into a series of smaller\n",
      "Training Step 56/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.59 ms /    90 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 69 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  other constraints. This triangle represents the fundamental framework for managing projects.\n",
      "The project management\n",
      "Training Step 57/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1234.83 ms /    84 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 17 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  relationships between team members. Identify the project resource allocation plan and the budget required for\n",
      "Training Step 58/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     726.27 ms /    32 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 66 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "Design project integration management\n",
      "Develop project integration management\n",
      "Acquire project integration management\n",
      "Training Step 59/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1232.50 ms /    81 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 65 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  approach, as well as the roles and responsibilities of the project team, stakeholders,\n",
      "Training Step 60/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1178.80 ms /    80 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  The purpose of a risk response plan is to:\n",
      "1. Identify and assess potential\n",
      "Training Step 61/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1370.44 ms /    88 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 35 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: holder expectations.\n",
      "\n",
      "Some of the key responsibilities of a project manager include:\n",
      "\n",
      "* Def\n",
      "Training Step 62/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     983.19 ms /    50 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  (1) setting up project parameters, (2) establishing monitoring procedures, (\n",
      "Training Step 63/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.97 ms /    83 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 73 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  To identify the stakeholders, follow the following steps:\n",
      "\n",
      "1. Define the project boundaries\n",
      "Training Step 64/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1259.01 ms /    88 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  \n",
      "\n",
      "1. Identify the project objectives: The first step in the risk assessment process\n",
      "Training Step 65/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     797.22 ms /    44 tokens\n",
      "Llama.generate: 5 prefix-match hit, remaining 22 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  The stages include initiation, planning, execution, and completion.\n",
      "The series of stages\n",
      "Training Step 66/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     702.82 ms /    37 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 18 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  \n",
      "*   Project plan and schedule\n",
      "*   Project charter\n",
      "*   Project\n",
      "Training Step 67/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     678.15 ms /    33 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:   Use cost accounting data and statistical methods to estimate the costs of the project.\n",
      "\n",
      "Training Step 68/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     657.26 ms /    31 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  and evaluate the likelihood and impact of those risks.\n",
      "Analyze the project schedule and\n",
      "Training Step 69/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     660.98 ms /    31 tokens\n",
      "Llama.generate: 4 prefix-match hit, remaining 32 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  is a method used to manage the planning, execution, and review of a software\n",
      "Training Step 70/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     809.67 ms /    47 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  involves assessing potential risks associated with the project and taking appropriate actions to mitigate or eliminate\n",
      "Training Step 71/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     768.06 ms /    44 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 52 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  This involves using the project management plan to identify the necessary steps, resources, and\n",
      "Training Step 72/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    52 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     993.21 ms /    67 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 30 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Here's how they can do it:\n",
      "\n",
      "1. **Recognize their contributions**:\n",
      "Training Step 73/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     792.62 ms /    45 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 67 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  that the project will be completed on or before the end date of the project.\n",
      "\n",
      "\n",
      "Training Step 74/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1212.08 ms /    82 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 25 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  line or a series of events on a chart or timeline.\n",
      "\n",
      "On the other hand\n",
      "Training Step 75/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     792.64 ms /    40 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 66 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  This will help in identifying roles and responsibilities. The following structure will be used:\n",
      "\n",
      "Training Step 76/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1255.43 ms /    81 tokens\n",
      "Llama.generate: 3 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  A project charter serves as the foundation for the project plan and other supporting documents.\n",
      "\n",
      "\n",
      "Training Step 77/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1213.69 ms /    83 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 33 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  and ensures that all the stakeholders are informed about the status of the project.\n",
      "\n",
      "The\n",
      "Training Step 78/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    33 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     828.27 ms /    48 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 25 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  This report should include the following:\n",
      "* Project scope\n",
      "* Assumptions\n",
      "\n",
      "Training Step 79/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     774.13 ms /    40 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 12 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: .\n",
      "\n",
      "*   **Example Use Case:** A team at XYZ Corporation wants to standard\n",
      "Training Step 80/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     613.09 ms /    27 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 57 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: : These are the management processes that deal with measuring and analyzing the performance of an\n",
      "Training Step 81/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1070.36 ms /    72 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 18 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Analyze the benefits and drawbacks of implementing different quality assurance processes and choose the most\n",
      "Training Step 82/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     711.57 ms /    33 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 66 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "Analyze the project scope\n",
      "Gather requirements\n",
      "Prioritize the requirements\n",
      "\n",
      "Training Step 83/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1195.98 ms /    81 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 61 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  regularly as the project progresses.\n",
      "\n",
      "## Step 1: Define the project scope\n",
      "\n",
      "Training Step 84/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1172.36 ms /    76 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 60 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  of what the project needs to address. For instance, a project to build a\n",
      "Training Step 85/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1131.94 ms /    75 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 13 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Here is a sample project organization structure for a software development project:\n",
      "\n",
      "**Project Name\n",
      "Training Step 86/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     625.48 ms /    28 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 17 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "Agreement\n",
      "Quote\n",
      "Proposal\n",
      "RFP\n",
      "Procurement plan\n",
      "\n",
      "The\n",
      "Training Step 87/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     668.86 ms /    32 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 9 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  and deliverables\n",
      "Establish clear requirements and specifications\n",
      "Conduct stakeholder analysis\n",
      "\n",
      "Training Step 88/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     588.81 ms /    24 tokens\n",
      "Llama.generate: 2 prefix-match hit, remaining 56 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  (IMP) is a systematic approach to identifying, designing, implementing, testing,\n",
      "Training Step 89/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1091.04 ms /    71 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 19 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  It also includes the detailed project scope statement, including all major components of the project\n",
      "Training Step 90/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     676.57 ms /    34 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 71 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  \n",
      "\n",
      "*   Assessing the project's scope, timeline, budget, and resources\n",
      "Training Step 91/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1240.40 ms /    86 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 28 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ables, and documentation.\n",
      "\n",
      "On the other hand, scope validation is a more formal\n",
      "Training Step 92/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     796.03 ms /    43 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 47 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: .\n",
      "\n",
      "* This option is not a reason the project manager might use a risk register\n",
      "Training Step 93/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    47 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1140.10 ms /    62 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  The WBS should be tailored to a specific project related to building and installing a\n",
      "Training Step 94/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     278.39 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1493.17 ms /    87 tokens\n",
      "Llama.generate: 4 prefix-match hit, remaining 20 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  responsible for coordinating the project's budget with the client and stakeholders, and ensuring that\n",
      "Training Step 95/300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(formatted_prompts):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(formatted_prompts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     output \u001b[38;5;241m=\u001b[39m llama(text)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetuning complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:1900\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m   1837\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1838\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1863\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[0;32m   1865\u001b[0m \n\u001b[0;32m   1866\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1898\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1900\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_completion(\n\u001b[0;32m   1901\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   1902\u001b[0m         suffix\u001b[38;5;241m=\u001b[39msuffix,\n\u001b[0;32m   1903\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[0;32m   1904\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1905\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1906\u001b[0m         min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1907\u001b[0m         typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1908\u001b[0m         logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[0;32m   1909\u001b[0m         echo\u001b[38;5;241m=\u001b[39mecho,\n\u001b[0;32m   1910\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m   1911\u001b[0m         frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1912\u001b[0m         presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1913\u001b[0m         repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1914\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1915\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1916\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   1917\u001b[0m         tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1918\u001b[0m         mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1919\u001b[0m         mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1920\u001b[0m         mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1921\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1922\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1923\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1924\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1925\u001b[0m         logit_bias\u001b[38;5;241m=\u001b[39mlogit_bias,\n\u001b[0;32m   1926\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:1833\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1831\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[1;32m-> 1833\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:1318\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1316\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1317\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1319\u001b[0m     prompt_tokens,\n\u001b[0;32m   1320\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1321\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1322\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1323\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1324\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1325\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1326\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1327\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1328\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1329\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1330\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1331\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1332\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1333\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1334\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1335\u001b[0m ):\n\u001b[0;32m   1336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel, token):\n\u001b[0;32m   1337\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens, prev_tokens\u001b[38;5;241m=\u001b[39mprompt_tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:910\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 910\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[0;32m    912\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m    913\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    914\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    928\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[0;32m    929\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:643\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    639\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[0;32m    641\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[0;32m    642\u001b[0m )\n\u001b[1;32m--> 643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch)\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\_internals.py:300\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[1;32m--> 300\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[0;32m    302\u001b[0m         batch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 2: Define Training Loop for Fine-Tuning\n",
    "for step, text in enumerate(formatted_prompts):\n",
    "    \n",
    "    print(f\"Training Step {step+1}/{len(formatted_prompts)}\")\n",
    "    \n",
    "    \n",
    "    output = llama(text)\n",
    "    print(\"Output:\", output[\"choices\"][0][\"text\"])\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"finetuning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity input and output generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_context(activity_input):\n",
    "    # Lowercase input for case-insensitive matching\n",
    "    input_keywords = set(activity_input.lower().split())\n",
    "\n",
    "    # Try to find process IDs by matching keywords in `name` or `description`\n",
    "    matching_process_ids = []\n",
    "    for pid, data in G.nodes(data=True):\n",
    "        name_keywords = set(data.get(\"name\", \"\").lower().split())\n",
    "        description_keywords = set(data.get(\"description\", \"\").lower().split())\n",
    "        \n",
    "        # Check if there’s any overlap between input keywords and node keywords\n",
    "        if input_keywords & name_keywords or input_keywords & description_keywords:\n",
    "            matching_process_ids.append(pid)\n",
    "    \n",
    "    if not matching_process_ids:\n",
    "        print(\"No exact match found for the input. Attempting fallback context.\")\n",
    "        return \"No relevant process found in the knowledge graph.\"\n",
    "\n",
    "    process_id = matching_process_ids[0]\n",
    "    \n",
    "    # Filter out any numerical-only dependencies, stakeholders, or risks\n",
    "    dependencies = [dep for dep in get_dependencies(process_id) if isinstance(dep, str)]\n",
    "    stakeholders = [stake for stake in get_stakeholders(process_id) if isinstance(stake, str)]\n",
    "    risks = [risk for risk in get_risks(process_id) if isinstance(risk, str)]\n",
    "\n",
    "    # Construct context for Llama's input prompt\n",
    "    context = (\n",
    "        f\"Activity: {activity_input}\\n\"\n",
    "        f\"Dependencies: {dependencies}\\n\"\n",
    "        f\"Stakeholders: {stakeholders}\\n\"\n",
    "        f\"Risks: {risks}\\n\"\n",
    "    )\n",
    "\n",
    "    # Debug message for constructed context\n",
    "    print(f\"Constructed graph context for Llama:\\n{context}\")\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies for 'Direct and Manage Project Work': ['Work performance data', 'Work performance information', 'Team performance reports', 'Project Management Plan', 'Assignments', 'Agreements', 'Project Documents', 'Issues', 'Organizational Process Assets', '7', 'Communications Management Plan', '8', '9']\n",
      "Stakeholders for 'Direct and Manage Project Work': [49, 108, 24, 119, 9]\n",
      "Risks affecting 'Direct and Manage Project Work': [85, 5, 62, 59, 139, 76, 107, 10, 140, 118, 17, 57, 30, 1141, 1031]\n",
      "Constructed graph context for Llama:\n",
      "Activity: Generate a 7-day plan for developing a mobile app with a team of 4. Outline tasks, milestones, deliverables, and identify potential risks for each day, along with mitigation strategies. Ensure the plan is concise and brief.\n",
      "Dependencies: ['Work performance data', 'Work performance information', 'Team performance reports', 'Project Management Plan', 'Assignments', 'Agreements', 'Project Documents', 'Issues', 'Organizational Process Assets', '7', 'Communications Management Plan', '8', '9']\n",
      "Stakeholders: []\n",
      "Risks: []\n",
      "\n",
      "Final Prompt to Llama:\n",
      " Activity: Generate a 7-day plan for developing a mobile app with a team of 4. Outline tasks, milestones, deliverables, and identify potential risks for each day, along with mitigation strategies. Ensure the plan is concise and brief.\n",
      "Dependencies: ['Work performance data', 'Work performance information', 'Team performance reports', 'Project Management Plan', 'Assignments', 'Agreements', 'Project Documents', 'Issues', 'Organizational Process Assets', '7', 'Communications Management Plan', '8', '9']\n",
      "Stakeholders: []\n",
      "Risks: []\n",
      "\n",
      "\n",
      "Provide a detailed 7-day mobile app development plan. Outline each day’s tasks, key milestones, and expected deliverables. Include a brief risk assessment with potential risks and mitigation strategies. Keep responses concise and brief.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 159 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2434.80 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   352 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   17254.33 ms /   353 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response:  The plan includes dependencies and stakeholders.\n",
      "\n",
      "\n",
      "Day 1: Project Kick-off (16th of April)\n",
      "- Objective: Define project scope, assign tasks, and identify dependencies.\n",
      "- Tasks:\n",
      "\t+ Project kick-off meeting\n",
      "\t+ Establish a project management plan\n",
      "\t+ Define project scope\n",
      "\t+ Identify dependencies\n",
      "- Deliverables:\n",
      "\t+ Project management plan\n",
      "\t+ Project scope document\n",
      "- Risks:\n",
      "\t+ Unprepared team members\n",
      "\t+ Insufficient communication\n",
      "- Mitigation strategies:\n",
      "\t+ Prepare team members well\n",
      "\t+ Encourage open communication\n",
      "\n",
      "Day 2: Define Project Scope (17th of April)\n",
      "- Objective: Identify key features and requirements for the mobile app.\n",
      "- Tasks:\n",
      "\t+ Conduct a workshop to gather requirements\n",
      "\t+ Identify key features and requirements\n",
      "\t+ Create a requirements gathering document\n",
      "- Deliverables:\n",
      "\t+ Requirements gathering document\n",
      "\t+ List of key features and requirements\n",
      "- Risks:\n",
      "\t+ Insufficient requirement gathering\n",
      "\t+ Key features may not align with business goals\n",
      "- Mitigation strategies:\n",
      "\t+ Conduct a thorough requirement gathering process\n",
      "\t+ Ensure key features align with business goals\n",
      "\n",
      "Day 3: Design Mobile App (18th of April)\n",
      "- Objective: Create wireframes, prototypes, and high-fidelity designs for the mobile app.\n",
      "- Tasks:\n",
      "\t+ Conduct user research\n",
      "\t+ Create wireframes and prototypes\n",
      "\t+ Develop a visual design concept\n",
      "- Deliverables:\n",
      "\t+ Wireframes and prototypes\n",
      "\t+ Visual design concept\n",
      "- Risks:\n",
      "\t+ Insufficient user research\n",
      "\t+ Wireframes and prototypes may not meet requirements\n",
      "- Mitigation strategies:\n",
      "\t+ Conduct thorough user research\n",
      "\t+ Ensure wireframes and prototypes meet requirements\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_response_with_graph(activity_input):\n",
    "    # Retrieve context from the graph\n",
    "    graph_context = generate_graph_context(activity_input)\n",
    "\n",
    "    # Combine graph context with the activity description from `activity_input`\n",
    "    prompt = (\n",
    "        f\"{graph_context}\\n\\n\"\n",
    "        f\"{activity_input} Include a brief risk assessment with potential risks and mitigation strategies. \"\n",
    "        \"Keep responses concise and brief.\"\n",
    "    )\n",
    "    \n",
    "    # Print the final prompt sent to Llama for debugging\n",
    "    print(\"Final Prompt to Llama:\\n\", prompt)\n",
    "\n",
    "    # Generate response using Llama with the new prompt\n",
    "    response = llama(prompt, max_tokens=5500)\n",
    "\n",
    "    # Extract the text from the response and print it\n",
    "    output_text = response[\"choices\"][0][\"text\"]\n",
    "    print(\"Model Response:\", output_text)\n",
    "    return output_text\n",
    "\n",
    "# Run the updated function with dynamic activity_input\n",
    "Fine_tuned_Response = generate_response_with_graph(activity_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Relevant Information from Llama’s Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_entities_from_output(output_text):\n",
    "    \"\"\"\n",
    "    Extract tasks, dependencies, risks, and stakeholders from Llama's output.\n",
    "    This function uses regex to identify key phrases for simplicity.\n",
    "    \"\"\"\n",
    "    tasks = re.findall(r\"Tasks?: (.+?)(?:\\n|$)\", output_text)\n",
    "    dependencies = re.findall(r\"Dependencies?: (.+?)(?:\\n|$)\", output_text)\n",
    "    risks = re.findall(r\"Risks?: (.+?)(?:\\n|$)\", output_text)\n",
    "    stakeholders = re.findall(r\"Stakeholders?: (.+?)(?:\\n|$)\", output_text)\n",
    "\n",
    "    # Debug output for extracted entities\n",
    "    print(\"Extracted Tasks:\", tasks)\n",
    "    print(\"Extracted Dependencies:\", dependencies)\n",
    "    print(\"Extracted Risks:\", risks)\n",
    "    print(\"Extracted Stakeholders:\", stakeholders)\n",
    "    \n",
    "    return tasks, dependencies, risks, stakeholders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Ponderation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keyword sets for relevance scoring\n",
    "task_keywords = {\"task\", \"milestone\", \"deliverable\", \"complete\"}\n",
    "dependency_keywords = {\"dependency\", \"requirement\", \"before\", \"after\"}\n",
    "risk_keywords = {\"risk\", \"issue\", \"challenge\", \"mitigation\"}\n",
    "stakeholder_keywords = {\"stakeholder\", \"team\", \"manager\", \"client\", \"involved\"}\n",
    "\n",
    "def score_entity(entity, keywords):\n",
    "    \"\"\"\n",
    "    Assign a relevance score based on the presence of keywords.\n",
    "    Higher scores for entities with multiple relevant keywords.\n",
    "    \"\"\"\n",
    "    words = set(entity.lower().split())\n",
    "    score = sum(1 for word in words if word in keywords)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ponderate_entities(entities, keywords):\n",
    "    \"\"\"\n",
    "    Filter and rank entities based on contextual relevance using keyword scoring.\n",
    "    \"\"\"\n",
    "    scored_entities = [(entity, score_entity(entity, keywords)) for entity in entities]\n",
    "    # Filter out entities with a score of 0 (no relevance)\n",
    "    scored_entities = [(entity, score) for entity, score in scored_entities if score > 0]\n",
    "    # Sort by score in descending order\n",
    "    ranked_entities = sorted(scored_entities, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Debug output for scored entities\n",
    "    print(\"Scored and Ranked Entities:\", ranked_entities)\n",
    "    \n",
    "    # Return only the entity names, not the scores, for final output\n",
    "    return [entity for entity, score in ranked_entities]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Graph expansion if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_graph_with_llama_output(output_text):\n",
    "    # Extract entities from Llama’s output\n",
    "    tasks, dependencies, risks, stakeholders = extract_entities_from_output(output_text)\n",
    "    \n",
    "    # Ponderate each category with contextual scoring\n",
    "    tasks = ponderate_entities(tasks, task_keywords)\n",
    "    dependencies = ponderate_entities(dependencies, dependency_keywords)\n",
    "    risks = ponderate_entities(risks, risk_keywords)\n",
    "    stakeholders = ponderate_entities(stakeholders, stakeholder_keywords)\n",
    "    \n",
    "    # Add tasks as new nodes\n",
    "    for task in tasks:\n",
    "        G.add_node(task, type=\"task\")\n",
    "        print(f\"Added task node: {task}\")\n",
    "    \n",
    "    # Add dependencies as edges between tasks if applicable\n",
    "    for dependency in dependencies:\n",
    "        task_links = dependency.split(\" and \")\n",
    "        if len(task_links) == 2:\n",
    "            G.add_edge(task_links[0].strip(), task_links[1].strip(), relationship=\"dependency\")\n",
    "            print(f\"Added dependency edge: {task_links[0].strip()} -> {task_links[1].strip()}\")\n",
    "    \n",
    "    # Add risks and link them to tasks\n",
    "    for risk in risks:\n",
    "        G.add_node(risk, type=\"risk\")\n",
    "        for task in tasks:\n",
    "            G.add_edge(risk, task, relationship=\"risk_impact\")\n",
    "            print(f\"Added risk impact edge: {risk} -> {task}\")\n",
    "    \n",
    "    # Add stakeholders and link them to tasks\n",
    "    for stakeholder in stakeholders:\n",
    "        G.add_node(stakeholder, type=\"stakeholder\")\n",
    "        for task in tasks:\n",
    "            G.add_edge(stakeholder, task, relationship=\"involvement\")\n",
    "            print(f\"Added stakeholder involvement edge: {stakeholder} -> {task}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies for 'Direct and Manage Project Work': ['Work performance data', 'Work performance information', 'Team performance reports', 'Project Management Plan', 'Assignments', 'Agreements', 'Project Documents', 'Issues', 'Organizational Process Assets', '7', 'Communications Management Plan', '8', '9']\n",
      "Stakeholders for 'Direct and Manage Project Work': [49, 108, 24, 119, 9]\n",
      "Risks affecting 'Direct and Manage Project Work': [85, 5, 62, 59, 139, 76, 107, 10, 140, 118, 17, 57, 30, 1141, 1031]\n",
      "Constructed graph context for Llama:\n",
      "Activity: Identify additional tasks and risks for the mobile app project lifecycle\n",
      "Dependencies: ['Work performance data', 'Work performance information', 'Team performance reports', 'Project Management Plan', 'Assignments', 'Agreements', 'Project Documents', 'Issues', 'Organizational Process Assets', '7', 'Communications Management Plan', '8', '9']\n",
      "Stakeholders: []\n",
      "Risks: []\n",
      "\n",
      "Final Prompt to Llama:\n",
      " Activity: Identify additional tasks and risks for the mobile app project lifecycle\n",
      "Dependencies: ['Work performance data', 'Work performance information', 'Team performance reports', 'Project Management Plan', 'Assignments', 'Agreements', 'Project Documents', 'Issues', 'Organizational Process Assets', '7', 'Communications Management Plan', '8', '9']\n",
      "Stakeholders: []\n",
      "Risks: []\n",
      "\n",
      "\n",
      "Provide a detailed 7-day mobile app development plan. Outline each day’s tasks, key milestones, and expected deliverables. Include a brief risk assessment with potential risks and mitigation strategies. Keep responses concise and brief.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 124 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2434.80 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   387 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19396.88 ms /   388 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response:  Here is the plan:\n",
      "\n",
      "Day 1: Project Introduction and Planning (Duration: 1 hour)\n",
      "\n",
      "* Task:\n",
      "\t+ Introduce the project stakeholders, including project manager, team members, and external partners (as needed).\n",
      "\t+ Create a project scope statement outlining the project's objectives, deliverables, and boundaries.\n",
      "\t+ Define project timelines, milestones, and key performance indicators (KPIs).\n",
      "* Key Milestones:\n",
      "\t+ Project scope statement creation\n",
      "\t+ Timelines and milestones establishment\n",
      "* Expected Deliverables:\n",
      "\t+ Project scope statement document\n",
      "\t+ Timelines and milestones presentation\n",
      "* Risk Assessment:\n",
      "\t+ Potential risk: Project scope is not fully understood, leading to scope creep and delays.\n",
      "\t+ Mitigation Strategy: Regular communication with stakeholders, clear definition of project scope, and regular progress updates.\n",
      "* Task:\n",
      "\t+ Identify and prioritize project dependencies.\n",
      "\t+ Create a project management plan outline.\n",
      "\t+ Outline resource allocation and allocation plans.\n",
      "* Key Milestones:\n",
      "\t+ Task completion (30 minutes)\n",
      "\t+ Milestone establishment (45 minutes)\n",
      "* Expected Deliverables:\n",
      "\t+ Project management plan document\n",
      "\t+ Resource allocation plans\n",
      "* Risk Assessment:\n",
      "\t+ Potential risk: Insufficient resource allocation, leading to delays and resource constraints.\n",
      "\t+ Mitigation Strategy: Regular resource allocation review, flexible resource allocation plans, and communication with stakeholders.\n",
      "* Task:\n",
      "\t+ Define project deliverables and work performance data.\n",
      "\t+ Identify and categorize project issues.\n",
      "\t+ Create a project communications plan.\n",
      "* Key Milestones:\n",
      "\t+ Task completion (60 minutes)\n",
      "\t+ Deliverables definition (60 minutes)\n",
      "\t+ Issue identification and categorization (30 minutes)\n",
      "* Expected Deliverables:\n",
      "\t+ Project deliverables document\n",
      "\t+ Issue categorization\n",
      "\t+ Project communications plan\n",
      "* Risk Assessment:\n",
      "\t+ Potential risk: Insufficient deliverables, leading to project delays and scope creep.\n",
      "\n",
      "Extracted Tasks: []\n",
      "Extracted Dependencies: []\n",
      "Extracted Risks: []\n",
      "Extracted Stakeholders: []\n",
      "Scored and Ranked Entities: []\n",
      "Scored and Ranked Entities: []\n",
      "Scored and Ranked Entities: []\n",
      "Scored and Ranked Entities: []\n"
     ]
    }
   ],
   "source": [
    "# Example: Generate a response and expand the graph with new information\n",
    "activity_input = \"Identify additional tasks and risks for the mobile app project lifecycle\"\n",
    "response_text = generate_response_with_graph(activity_input)  # Assuming this generates a response\n",
    "\n",
    "# Expand the graph using Llama's output\n",
    "expand_graph_with_llama_output(response_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refinig output with a more perfomrant models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined Output from Groq API: Here is a refined project plan with a more realistic timeline, well-structured tasks, and a focus on stakeholder engagement:\n",
      "\n",
      "**Project Plan: Mobile App Development**\n",
      "\n",
      "**Project Kick-off (Week 1: 16-20 April)**\n",
      "\n",
      "* Objective: Set the project foundation, define scope, and identify dependencies.\n",
      "* Tasks:\n",
      "\t1. Project Kick-off Meeting (16th April): Invite team members, stakeholders, and clients to discuss project objectives, scope, and timelines.\n",
      "\t2. Establish Project Management Plan (16-17th April): Define project organization, roles, and responsibilities.\n",
      "\t3. Define Project Scope (17-18th April): Review business goals, identify key features and requirements, and create a scope statement.\n",
      "\t4. Identify Dependencies and Stakeholders (18-20th April): Document critical dependencies, identified stakeholders, and their roles.\n",
      "* Deliverables:\n",
      "\t+ Project Management Plan\n",
      "\t+ Project Scope Statement\n",
      "\t+ Initial Stakeholder Register\n",
      "* Risks:\n",
      "\t+ Unprepared team members\n",
      "\t+ Insufficient communication\n",
      "* Mitigation strategies:\n",
      "\t+ Prepare team members well\n",
      "\t+ Encourage open communication and collaboration\n",
      "\n",
      "**Requirements Gathering (Week 2: 23-27 April)**\n",
      "\n",
      "* Objective: Collect and document key features and requirements for the mobile app.\n",
      "* Tasks:\n",
      "\t1. Requirements Gathering Workshop (23-24th April): Convene a workshop with stakeholders, team members, and clients to discuss requirements and create a list of key features and requirements.\n",
      "\t2. Document Requirements (25-26th April): Create a comprehensive requirements gathering document and review with stakeholders.\n",
      "* Deliverables:\n",
      "\t+ Requirements Gathering Document\n",
      "\t+ List of Key Features and Requirements\n",
      "* Risks:\n",
      "\t+ Insufficient requirement gathering\n",
      "\t+ Key features may not align with business goals\n",
      "* Mitigation strategies:\n",
      "\t+ Conduct a thorough requirement gathering process\n",
      "\t+ Ensure key features align with business goals\n",
      "\n",
      "**Design and Prototyping (Week 3: 30 April - 4 May)**\n",
      "\n",
      "* Objective: Create wireframes, prototypes, and high-fidelity designs for the mobile app.\n",
      "* Tasks:\n",
      "\t1. User Research (30th April - 1st May): Conduct user research to inform design decisions.\n",
      "\t2. Design Creation (2-3rd May): Create wireframes, prototypes, and visual design concepts.\n",
      "\t3. Design Review (4th May): Review design concepts with stakeholders and team members.\n",
      "* Deliverables:\n",
      "\t+ Wireframes and Prototypes\n",
      "\t+ Visual Design Concept\n",
      "* Risks:\n",
      "\t+ Insufficient user research\n",
      "\t+ Wireframes and prototypes may not meet requirements\n",
      "* Mitigation strategies:\n",
      "\t+ Conduct thorough user research\n",
      "\t+ Ensure wireframes and prototypes meet requirements\n",
      "\n",
      "This refined plan:\n",
      "\n",
      "* Spreads tasks over more realistic weeks (allowing for more thorough execution)\n",
      "* Prioritizes tasks accordingly (e.g., requirements gathering before design)\n",
      "* Includes additional stakeholder engagement and review steps\n",
      "* Focuses on deliverables, risks, and mitigation strategies\n",
      "\n",
      "Remember to regularly review and adjust the plan as needed to ensure the project stays on track and aligned with stakeholder expectations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Loading API\n",
    "with open(\"pmkey.txt\", \"r\") as file:\n",
    "    api_key = file.read().strip()\n",
    "os.environ[\"GROQ_API_KEY\"] = api_key\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "# Define a function to refine the already-generated model output using Groq API\n",
    "def refine_with_groq(model_output):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Refine the following project plan to be more grounded in reality and well-structured:\\n\\n{model_output}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Send request to Groq API\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama3-8b-8192\" \n",
    "    )\n",
    "    \n",
    "    # Retrieve and print the refined output\n",
    "    refined_output = chat_completion.choices[0].message.content\n",
    "    print(\"Refined Output from Groq API:\", refined_output)\n",
    "    return refined_output\n",
    "\n",
    "# Call refine_with_groq to refine this stored response\n",
    "refined_output = refine_with_groq(Fine_tuned_Response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human evaluation and other Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: [{'rouge-1': {'r': 0.5333333333333333, 'p': 0.034334763948497854, 'f': 0.06451612789574664}, 'rouge-2': {'r': 0.0625, 'p': 0.002347417840375587, 'f': 0.004524886180053751}, 'rouge-l': {'r': 0.5333333333333333, 'p': 0.034334763948497854, 'f': 0.06451612789574664}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abder\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.5321035981178284\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "reference_text = \"\"\"\n",
    "Day 1: Define project scope, objectives, and team roles. \n",
    "Day 2: Requirements gathering and stakeholder analysis...\n",
    "\"\"\"  \n",
    "\n",
    "# ROUGE Score\n",
    "rouge = Rouge()\n",
    "rouge_scores = rouge.get_scores(refined_output, reference_text)\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n",
    "\n",
    "# Cosine Similarity\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "output_embedding = model.encode(refined_output, convert_to_tensor=True)\n",
    "reference_embedding = model.encode(reference_text, convert_to_tensor=True)\n",
    "cosine_similarity = util.pytorch_cos_sim(output_embedding, reference_embedding)\n",
    "print(\"Cosine Similarity:\", cosine_similarity.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
